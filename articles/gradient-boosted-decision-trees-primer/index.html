<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="height=device-height,width=device-width,initial-scale=1.0,user-scalable=no"><meta description="Andrew Tulloch - Machine Learning, Statistics, Systems"><meta keywords="andrew tulloch,tulloch,machine learning,statistics,mathematics,systems,programming"><title>A Primer on Gradient Boosted Decision Trees &mdash;Andrew Tulloch</title><link rel="alternate" href="https://tullo.ch/feed.xml" type="application/rss+xml" title="Machine Learning, Statistics, Systems"><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: { inlineMath: [['$','$'],['\\(','\\)']] },
  TeX: { equationNumbers: {autoNumber: "AMS"} }
});</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script><script type="text/javascript">(function(e,b){if(!b.__SV){var a,f,i,g;window.mixpanel=b;a=e.createElement("script");a.type="text/javascript";a.async=!0;a.src=("https:"===e.location.protocol?"https:":"http:")+'//cdn.mxpnl.com/libs/mixpanel-2.2.min.js';f=e.getElementsByTagName("script")[0];f.parentNode.insertBefore(a,f);b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==
typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");for(g=0;g<i.length;g++)f(c,i[g]);
b._i.push([a,e,d])};b.__SV=1.2}})(document,window.mixpanel||[]);
mixpanel.init("9883d2ddb1fd32faf91fa16afe22a008");
mixpanel.track('Page View', {'page name' : document.title, 'url' : window.location.pathname});
</script><script type="text/javascript">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-44466528-2', 'tullo.ch');
ga('send', 'pageview');
</script></head><body><header id="header"><div class="container"><div class="gravatar"><img class="gravatar" src="http://www.gravatar.com/avatar/c4ade7596c93b909699000666b47bc53?s=200"></div><div id="brand"><h1 class="site-title"><a class="blog-title" href="https://tullo.ch">Andrew Tulloch</a><span>â€”</span><span>Machine Learning, Statistics, Systems</span></h1></div><nav class="site-navigation main-navigation" role="navigation"><a href="/about/">About</a> | <a href="/academic/">Academic</a> | <a href="https://github.com/ajtulloch">GitHub</a> | <a href="/static/cv.pdf">CV</a></nav><div class="clear"></div></div></header><div class="container" id="main"><article class="post"><h1 class="title"><a href="/articles/gradient-boosted-decision-trees-primer/">A Primer on Gradient Boosted Decision Trees</a></h1><div class="post-meta"><p class="date">11 March 2013</p></div><div class="the-content"><section class="content"><p>Gradient boosted decision trees are an effective off-the-shelf method
for generating effective models for classification and regression
tasks.</p>
<p>Gradient boosting is a generic technique that can be applied to
arbitrary &#39;underlying&#39; weak learners - typically decision trees are
used. The seminal reference is <a href="https://en.wikipedia.org/wiki/Jerome_H._Friedman">Friedman&#39;s</a> <a href="http://statweb.stanford.edu/~jhf/ftp/trebst.pdf">2001 paper</a> that
introduced the method.</p>
<p>Consider a typical supervised learning problem - we are given as input
labeled feature vectors $(x, y)$, and seek to estimate a function
$\hat F(x)$ that approximates the &#39;true&#39; mapping $F^\star$, with
$F^\star$ minimizing the expected loss $L(y, F(x)$ over some candidate
functions $\mathcal{F}$ for a loss function $L$.</p>
<span class="more"/>

<p>In gradient boosting, the model assumes an additive expansion
\begin{equation} F(x, \beta, \alpha) = \sum_{i=1}^{n} \beta_{i} h(x,
\alpha_{i}) \end{equation} where the $h$ are our weak learners. Thus,
the predictor from gradient boosting is a linear combination of weak
learners, and the procedure does two things:</p>
<ul>
<li>Computes $\beta_m$ - the weight that a given classifier has in
context of the ensemble.</li>
<li>Weights the training examples to compute the $i$-th weak classifier
$h(\cdot, \alpha_m)$.</li>
</ul>
<h2 id="the-algorithm">The Algorithm</h2>
<p>For examples, we&#39;ll use the decision tree training library I wrote in
Go, available on GitHub at <a href="https://github.com/ajtulloch/decisiontrees">https://github.com/ajtulloch/decisiontrees</a>.</p>
<p>The boosting algorithm, in pseudo-code, is quite simple:</p>
<ul>
<li>initialize list of weak learners to a singleton list with simple
prior</li>
<li>for each round in 1..numRounds:</li>
<li>reweight examples $(x, y)$ to $(x, \tilde y)$ by &#39;upweighting&#39;
examples that the existing forest poorly predicts</li>
<li>estimate new weak classifier $h_i$ on weighted examples</li>
<li>compute weight $\beta_i$ of new weak classifier</li>
<li>add the pair $(h_i, \beta_i)$ to the forest</li>
<li>return forest</li>
</ul>
<p>The intuition behind gradient boosting is quite simple - we
iteratively build a sequence of predictors, and our final predictor is
a weighted average of these predictors. At each step, we focus on
adding an incremental classifier that improves the performance of the
entire ensemble. The technical description is &#39;gradient descent in
functional space&#39;.</p>
<p>Thus, if we have examples that are not well predicted by the current
ensemble, the next stage will work harder to fit these examples.</p>
<h2 id="example-implementation">Example Implementation</h2>
<p>For an implementation of the above approach, see the <a href="https://gist.github.com/ajtulloch/7254143">boosting.go</a>
file on GitHub. The loop body is a simple function inlined below:</p>
<script src="https://gist.github.com/ajtulloch/7254274.js"></script>

<p>There are some complications (stochastic gradient boosting, influence
trimming), although the core algorithm is as described in the
pseudocode.</p>
<p>In subsequent posts we&#39;ll elaborate on</p>
<ul>
<li>Algorithms for training the individual weak learners</li>
<li>Boosting and decision tree hyperparameters</li>
<li>Speeding up training and prediction</li>
</ul>
</section></div><div class="meta clearfix"></div><div class="comments"><div id="disqus_thread"></div><script type="text/javascript">
var disqus_shortname = 'tulloch';
(function() {
   var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
   dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
   (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a></div></article></div><footer><div class="container"><p class="copyright">&copy;2021 Andrew Tulloch</p></div></footer></body></html>