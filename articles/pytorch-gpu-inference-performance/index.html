<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="height=device-height,width=device-width,initial-scale=1.0,user-scalable=no"><meta description="Andrew Tulloch - Machine Learning, Statistics, Systems"><meta keywords="andrew tulloch,tulloch,machine learning,statistics,mathematics,systems,programming"><title>Improving PyTorch inference performance on GPUs with a few simple tricks &mdash;Andrew Tulloch</title><link rel="alternate" href="https://tullo.ch/feed.xml" type="application/rss+xml" title="Machine Learning, Statistics, Systems"><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: { inlineMath: [['$','$'],['\\(','\\)']] },
  TeX: { equationNumbers: {autoNumber: "AMS"} }
});</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script><script type="text/javascript">(function(e,b){if(!b.__SV){var a,f,i,g;window.mixpanel=b;a=e.createElement("script");a.type="text/javascript";a.async=!0;a.src=("https:"===e.location.protocol?"https:":"http:")+'//cdn.mxpnl.com/libs/mixpanel-2.2.min.js';f=e.getElementsByTagName("script")[0];f.parentNode.insertBefore(a,f);b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==
typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");for(g=0;g<i.length;g++)f(c,i[g]);
b._i.push([a,e,d])};b.__SV=1.2}})(document,window.mixpanel||[]);
mixpanel.init("9883d2ddb1fd32faf91fa16afe22a008");
mixpanel.track('Page View', {'page name' : document.title, 'url' : window.location.pathname});
</script><script type="text/javascript">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-44466528-2', 'tullo.ch');
ga('send', 'pageview');
</script></head><body><header id="header"><div class="container"><div class="gravatar"><img class="gravatar" src="http://www.gravatar.com/avatar/c4ade7596c93b909699000666b47bc53?s=200"></div><div id="brand"><h1 class="site-title"><a class="blog-title" href="https://tullo.ch">Andrew Tulloch</a><span>—</span><span>Machine Learning, Statistics, Systems</span></h1></div><nav class="site-navigation main-navigation" role="navigation"><a href="/about/">About</a> | <a href="/academic/">Academic</a> | <a href="https://github.com/ajtulloch">GitHub</a> | <a href="/static/cv.pdf">CV</a></nav><div class="clear"></div></div></header><div class="container" id="main"><article class="post"><h1 class="title"><a href="/articles/pytorch-gpu-inference-performance/">Improving PyTorch inference performance on GPUs with a few simple tricks</a></h1><div class="post-meta"><p class="date">3 October 2021</p></div><div class="the-content"><section class="content"><p>We hear a lot these days that <a href="https://dl.acm.org/doi/10.1145/3317550.3321441">&quot;machine learning systems are stuck in a rut&quot;</a>. This is might be expanded out as &quot;standard architectures run  at high efficiencies due to disproportionate engineering effort, while theoretically better but non-standard architectures run at low efficiencies due to the lack of specialized engineering work on these alternatives and the failure of our systems to provide generalized performance.&quot;</p>
<p>This is a reasonable thesis. But as long as it is the status quo, we may as well enjoy the empirical efficiencies from standard architectures. That is, less &quot;stuck-in-a-rut&quot; thinking and more &quot;pigs-at-a-trough&quot; thinking! If you&#39;re going to use vanilla architectures and not play with the fanciest modern variants, you may as well take advantage of their practical efficiency.</p>
<p>Out of the box, PyTorch doesn&#39;t always provide the best performance for these models. There are good (and bad) reasons for this, but regardless it&#39;s generally very simple to achieve very high <a href="https://en.wikipedia.org/wiki/Roofline_model">roofline</a> efficiency during inference with a few simple steps.</p>
<p>Here, I&#39;ll just cover two quick examples: convolutional neural networks (where ResNet-101 is an exemplar rut architecture) and Transformer encoders (where BERT-Large is an exemplar rut architecture), run in float16 on NVIDIA GPUs. This is mostly focusing on the sweet spot of modern ML systems - large(ish) models, NVIDIA GPUs, half-precision, reasonably large batch sizes – and with a little work the results are great.</p>
<h2 id="convolutional-neural-networks-resnet-101-">Convolutional Neural Networks (ResNet-101)</h2>
<p>The highest performance implementation of CNNs on NVIDIA GPUs these days is generally TensorRT.  TensorRT is a graph compilation library - users construct an old-school (i.e. the way we did things roughly 2 years ago) computation graph, and then hand it to TensorRT, which applies a bunch of optimizations (horizontal and vertical kernel fusion, kernel selection, memory planning, and so on).</p>
<p>There are a number of ways to go from a PyTorch model to a TensorRT graph. These include <a href="https://github.com/NVIDIA-AI-IOT/torch2trt"><code>torch2trt</code></a>, <a href="https://github.com/pytorch/pytorch/tree/master/torch/fx/experimental/fx2trt"><code>fx2trt</code></a>, <a href="https://github.com/NVIDIA/TRTorch/">TRTorch</a>, and <a href="https://pytorch.org/docs/stable/onnx.html"><code>torch.onnx.export</code></a> followed by <a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#trtexec-ovr"><code>trtexec</code></a>.</p>
<p>These approaches all end up in roughly the same place - a new <code>nn.Module</code> instance which dispatches to the compiled TensorRT engine.  With <code>torch2trt</code>, this is literally a one-liner: <code>model = torch2trt(model, inputs)</code>.</p>
<p>Since this is right in the sweet spot of the NVIDIA stack (a huge amount of dedicated time has been spent making this workload fast), performance is great, achieving roughly 160TFLOP/s on an A100 GPU with TensorRT 8.0, and roughly 4x faster than the naive PyTorch implementation.</p>
<p><a href="/articles/pytorch-gpu-inference-performance/resnet101.png"><img src="/articles/pytorch-gpu-inference-performance/resnet101.png" alt=""></a></p>
<p><a href="https://gist.github.com/ajtulloch/eb268da0ca7ca57d1c239460019183d8">https://gist.github.com/ajtulloch/eb268da0ca7ca57d1c239460019183d8</a> is a fully-worked example.</p>
<h2 id="transformer-encoders-bert-large-">Transformer Encoders (BERT-Large)</h2>
<p>Transformer encoders require just a few components to be well implemented to be reasonably fast: <code>nn.TransformerEncoderLayer</code>, <code>nn.MultiHeadAttention</code>, and so on if you&#39;re familiar with <code>torch.nn</code>, or the code underlying <code>transformers.BertModel</code> if you&#39;re more familiar with HuggingFace transformers. Again for some good and bad reasons, vanilla PyTorch doesn&#39;t implement these in the most performance-optimal way.</p>
<p>Luckily, there are a large number of libraries that have produced high-quality and performance tuned implementations of these primitives for inference. These include <a href="https://github.com/NVIDIA/FasterTransformer">NVIDIA FasterTransformer</a>, <a href="https://github.com/bytedance/lightseq">ByteDance LightSeq</a>, <a href="https://github.com/microsoft/DeepSpeed">Microsoft DeepSpeed</a>, and many more.</p>
<p>Using these optimized runtimes with an existed trained model in a framework like <a href="https://github.com/pytorch/fairseq"><code>fairseq</code></a>, <a href="https://huggingface.co/transformers/">HuggingFace transformers</a>, or a <a href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html"><code>nn.TransformerEncoder</code></a> is a mildly annoying but generally worthwhile exercise - extract out the weights from the vanilla model, reformat them for the target inference runtime, instantiate the new <code>nn.Module</code>, and swap out the old encoder for the optimized one.</p>
<p>There&#39;s really not much more to it than that. If you do this, you might expect roughly 2-4x improvements compared to a naive implementation, and again get to roughly 160TFLOP/s or more.</p>
<p><a href="/articles/pytorch-gpu-inference-performance/bertlarge.png"><img src="/articles/pytorch-gpu-inference-performance/bertlarge.png" alt=""></a></p>
<p><a href="https://gist.github.com/ajtulloch/9c1e051e7389027d5d455c7afc052340">https://gist.github.com/ajtulloch/9c1e051e7389027d5d455c7afc052340</a> is a fully worked example of going between a HuggingFace BERT-Large transformer encoder and an equivalent FasterTransformer encoder, and <a href="https://dev-discuss.pytorch.org/t/making-transformer-inference-faster-on-gpus/190">this post I wrote</a> goes into more detail.</p>
<h2 id="keeping-gpus-busy">Keeping GPUs busy</h2>
<p>The high roofline efficiencies quoted here occur at reasonably large batch sizes.  There&#39;s no real way of getting around it – GPUs need a reasonable amount of concurrent work to saturate the increasing number of SMs (up to 108 on the latest A100 GPU) and keep the tensor cores busy. There are a few complementary ways to achieve this in practice: use relatively wide models (where the non-batched dimensions are large), use batching, and use multiple streams at once. All of these help to improve the effective amount of work active on the GPU at once, and drive up achieved utilization.</p>
<p>In terms of increasing the effective amount of work at inference time, a good place to start would be to use a dynamic batching queue (first popularized by <a href="https://arxiv.org/abs/1512.02595">DeepSpeech 2</a>) and implemented in <a href="https://pytorch.org/serve/batch_inference_with_ts.html">TorchServe</a>, <a href="https://github.com/triton-inference-server/server">Triton</a>, and many others. Then, look at increasing the number of streams, and ensure you aren&#39;t bottlenecked on the I/O or pre/post-processing.</p>
<p>As a rough guide to improving the inference efficiency of standard architectures on PyTorch:</p>
<ol>
<li>Ensure you are using half-precision on GPUs with <code>model.cuda().half()</code></li>
<li>Ensure the whole model runs on the GPU, without a lot of host-to-device or device-to-host transfers.</li>
<li>Ensure you are running with a reasonably large batch size.</li>
<li>Ensure the input pipeline is reasonable (no egregiously slow Python pre/post-processing, etc).</li>
<li>Ensure the GPU kernels are well-optimized (e.g. enable TensorRT for CNNs, enable FasterTransformer for Transformers).</li>
</ol>
<p>If you follow these steps, you should get very high efficiency on modern hardware. Occasionally there are upsides to our field being stuck in a rut :)</p>
</section></div><div class="meta clearfix"></div><div class="comments"><div id="disqus_thread"></div><script type="text/javascript">
var disqus_shortname = 'tulloch';
(function() {
   var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
   dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
   (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a></div></article></div><footer><div class="container"><p class="copyright">&copy;2021 Andrew Tulloch</p></div></footer></body></html>